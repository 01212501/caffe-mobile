--- caffe/src/caffe/net.cpp	2017-01-27 09:51:56.935642700 +0800
+++ ../src/caffe/net.cpp	2017-02-04 22:35:34.654099433 +0800
@@ -5,19 +5,27 @@
 #include <utility>
 #include <vector>
 
+#ifdef USE_HDF5
 #include "hdf5.h"
+#endif
 
 #include "caffe/common.hpp"
 #include "caffe/layer.hpp"
 #include "caffe/net.hpp"
+#ifdef NO_CAFFE_MOBILE
 #include "caffe/parallel.hpp"
+#endif
 #include "caffe/proto/caffe.pb.h"
+#ifdef USE_HDF5
 #include "caffe/util/hdf5.hpp"
+#endif
 #include "caffe/util/insert_splits.hpp"
 #include "caffe/util/math_functions.hpp"
 #include "caffe/util/upgrade_proto.hpp"
 
+#ifdef NO_CAFFE_MOBILE
 #include "caffe/test/test_caffe_main.hpp"
+#endif
 
 namespace caffe {
 
@@ -50,9 +58,11 @@
   // the current NetState.
   NetParameter filtered_param;
   FilterNet(in_param, &filtered_param);
+#ifdef USE_GLOG
   LOG_IF(INFO, Caffe::root_solver())
       << "Initializing net from parameters: " << std::endl
       << filtered_param.DebugString();
+#endif
   // Create a copy of filtered_param with splits added where necessary.
   NetParameter param;
   InsertSplits(filtered_param, &param);
@@ -83,8 +93,10 @@
     }
     layers_.push_back(LayerRegistry<Dtype>::CreateLayer(layer_param));
     layer_names_.push_back(layer_param.name());
+#ifdef USE_GLOG
     LOG_IF(INFO, Caffe::root_solver())
         << "Creating Layer " << layer_param.name();
+#endif
     bool need_backward = false;
 
     // Figure out this layer's input and output
@@ -121,23 +133,31 @@
     }
     // After this layer is connected, set it up.
     layers_[layer_id]->SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]);
+#ifdef USE_GLOG
     LOG_IF(INFO, Caffe::root_solver())
         << "Setting up " << layer_names_[layer_id];
+#endif
     for (int top_id = 0; top_id < top_vecs_[layer_id].size(); ++top_id) {
       if (blob_loss_weights_.size() <= top_id_vecs_[layer_id][top_id]) {
         blob_loss_weights_.resize(top_id_vecs_[layer_id][top_id] + 1, Dtype(0));
       }
       blob_loss_weights_[top_id_vecs_[layer_id][top_id]] = layer->loss(top_id);
+#ifdef USE_GLOG
       LOG_IF(INFO, Caffe::root_solver())
           << "Top shape: " << top_vecs_[layer_id][top_id]->shape_string();
+#endif
       if (layer->loss(top_id)) {
+#ifdef USE_GLOG
         LOG_IF(INFO, Caffe::root_solver())
             << "    with loss weight " << layer->loss(top_id);
+#endif
       }
       memory_used_ += top_vecs_[layer_id][top_id]->count();
     }
+#ifdef USE_GLOG
     LOG_IF(INFO, Caffe::root_solver())
         << "Memory required for data: " << memory_used_ * sizeof(Dtype);
+#endif
     const int param_size = layer_param.param_size();
     const int num_param_blobs = layers_[layer_id]->blobs().size();
     CHECK_LE(param_size, num_param_blobs)
@@ -241,8 +261,10 @@
   // In the end, all remaining blobs are considered output blobs.
   for (set<string>::iterator it = available_blobs.begin();
       it != available_blobs.end(); ++it) {
+#ifdef USE_GLOG
     LOG_IF(INFO, Caffe::root_solver())
         << "This network produces output " << *it;
+#endif
     net_output_blobs_.push_back(blobs_[blob_name_to_idx[*it]].get());
     net_output_blob_indices_.push_back(blob_name_to_idx[*it]);
   }
@@ -254,7 +276,9 @@
   }
   ShareWeights();
   debug_info_ = param.debug_info();
+#ifdef USE_GLOG
   LOG_IF(INFO, Caffe::root_solver()) << "Network initialization done.";
+#endif
 }
 
 template <typename Dtype>
@@ -293,30 +317,36 @@
   // Check whether the rule is broken due to phase.
   if (rule.has_phase()) {
       if (rule.phase() != state.phase()) {
+#ifdef USE_GLOG
         LOG_IF(INFO, Caffe::root_solver())
             << "The NetState phase (" << state.phase()
             << ") differed from the phase (" << rule.phase()
             << ") specified by a rule in layer " << layer_name;
+#endif
         return false;
       }
   }
   // Check whether the rule is broken due to min level.
   if (rule.has_min_level()) {
     if (state.level() < rule.min_level()) {
+#ifdef USE_GLOG
       LOG_IF(INFO, Caffe::root_solver())
           << "The NetState level (" << state.level()
           << ") is above the min_level (" << rule.min_level()
           << ") specified by a rule in layer " << layer_name;
+#endif
       return false;
     }
   }
   // Check whether the rule is broken due to max level.
   if (rule.has_max_level()) {
     if (state.level() > rule.max_level()) {
+#ifdef USE_GLOG
       LOG_IF(INFO, Caffe::root_solver())
           << "The NetState level (" << state.level()
           << ") is above the max_level (" << rule.max_level()
           << ") specified by a rule in layer " << layer_name;
+#endif
       return false;
     }
   }
@@ -329,9 +359,11 @@
       if (rule.stage(i) == state.stage(j)) { has_stage = true; }
     }
     if (!has_stage) {
+#ifdef USE_GLOG
       LOG_IF(INFO, Caffe::root_solver())
           << "The NetState did not contain stage '" << rule.stage(i)
           << "' specified by a rule in layer " << layer_name;
+#endif
       return false;
     }
   }
@@ -344,9 +376,11 @@
       if (rule.not_stage(i) == state.stage(j)) { has_stage = true; }
     }
     if (has_stage) {
+#ifdef USE_GLOG
       LOG_IF(INFO, Caffe::root_solver())
           << "The NetState contained a not_stage '" << rule.not_stage(i)
           << "' specified by a rule in layer " << layer_name;
+#endif
       return false;
     }
   }
@@ -366,8 +400,10 @@
   if (blob_name_to_idx && layer_param->bottom_size() > top_id &&
       blob_name == layer_param->bottom(top_id)) {
     // In-place computation
+#ifdef USE_GLOG
     LOG_IF(INFO, Caffe::root_solver())
         << layer_param->name() << " -> " << blob_name << " (in-place)";
+#endif
     top_vecs_[layer_id].push_back(blobs_[(*blob_name_to_idx)[blob_name]].get());
     top_id_vecs_[layer_id].push_back((*blob_name_to_idx)[blob_name]);
   } else if (blob_name_to_idx &&
@@ -405,8 +441,10 @@
                << layer_param.name() << "', bottom index " << bottom_id << ")";
   }
   const int blob_id = (*blob_name_to_idx)[blob_name];
+#ifdef USE_GLOG
   LOG_IF(INFO, Caffe::root_solver())
       << layer_names_[layer_id] << " <- " << blob_name;
+#endif
   bottom_vecs_[layer_id].push_back(blobs_[blob_id].get());
   bottom_id_vecs_[layer_id].push_back(blob_id);
   available_blobs->erase(blob_name);
@@ -464,10 +502,12 @@
         param_layer_indices_[owner_net_param_id];
     const int owner_layer_id = owner_index.first;
     const int owner_param_id = owner_index.second;
+#ifdef USE_GLOG
     LOG_IF(INFO, Caffe::root_solver()) << "Sharing parameters '" << param_name
         << "' owned by "
         << "layer '" << layer_names_[owner_layer_id] << "', param "
         << "index " << owner_param_id;
+#endif
     Blob<Dtype>* this_blob = layers_[layer_id]->blobs()[param_id].get();
     Blob<Dtype>* owner_blob =
         layers_[owner_layer_id]->blobs()[owner_param_id].get();
@@ -556,8 +596,10 @@
 template <typename Dtype>
 const vector<Blob<Dtype>*>& Net<Dtype>::Forward(
     const vector<Blob<Dtype>*> & bottom, Dtype* loss) {
+#ifdef USE_GLOG
   LOG_EVERY_N(WARNING, 1000) << "DEPRECATED: Forward(bottom, loss) "
       << "will be removed in a future version. Use Forward(loss).";
+#endif
   // Copy bottom to net bottoms
   for (int i = 0; i < bottom.size(); ++i) {
     net_input_blobs_[i]->CopyFrom(*bottom[i]);
@@ -565,6 +607,7 @@
   return Forward(loss);
 }
 
+#ifdef ENABLE_BACKWARD
 template <typename Dtype>
 void Net<Dtype>::BackwardFromTo(int start, int end) {
   CHECK_GE(end, 0);
@@ -583,6 +626,7 @@
     }
   }
 }
+#endif
 
 template <typename Dtype>
 void Net<Dtype>::ForwardDebugInfo(const int layer_id) {
@@ -590,11 +634,13 @@
     const Blob<Dtype>& blob = *top_vecs_[layer_id][top_id];
     const string& blob_name = blob_names_[top_id_vecs_[layer_id][top_id]];
     const Dtype data_abs_val_mean = blob.asum_data() / blob.count();
+#ifdef USE_GLOG
     LOG_IF(INFO, Caffe::root_solver())
         << "    [Forward] "
         << "Layer " << layer_names_[layer_id]
         << ", top blob " << blob_name
         << " data: " << data_abs_val_mean;
+#endif
   }
   for (int param_id = 0; param_id < layers_[layer_id]->blobs().size();
        ++param_id) {
@@ -602,14 +648,17 @@
     const int net_param_id = param_id_vecs_[layer_id][param_id];
     const string& blob_name = param_display_names_[net_param_id];
     const Dtype data_abs_val_mean = blob.asum_data() / blob.count();
+#ifdef USE_GLOG
     LOG_IF(INFO, Caffe::root_solver())
         << "    [Forward] "
         << "Layer " << layer_names_[layer_id]
         << ", param blob " << blob_name
         << " data: " << data_abs_val_mean;
+#endif
   }
 }
 
+#ifdef ENABLE_BACKWARD
 template <typename Dtype>
 void Net<Dtype>::BackwardDebugInfo(const int layer_id) {
   const vector<Blob<Dtype>*>& bottom_vec = bottom_vecs_[layer_id];
@@ -618,24 +667,29 @@
     const Blob<Dtype>& blob = *bottom_vec[bottom_id];
     const string& blob_name = blob_names_[bottom_id_vecs_[layer_id][bottom_id]];
     const Dtype diff_abs_val_mean = blob.asum_diff() / blob.count();
+#ifdef USE_GLOG
     LOG_IF(INFO, Caffe::root_solver())
         << "    [Backward] "
         << "Layer " << layer_names_[layer_id]
         << ", bottom blob " << blob_name
         << " diff: " << diff_abs_val_mean;
+#endif
   }
   for (int param_id = 0; param_id < layers_[layer_id]->blobs().size();
        ++param_id) {
     if (!layers_[layer_id]->param_propagate_down(param_id)) { continue; }
     const Blob<Dtype>& blob = *layers_[layer_id]->blobs()[param_id];
     const Dtype diff_abs_val_mean = blob.asum_diff() / blob.count();
+#ifdef USE_GLOG
     LOG_IF(INFO, Caffe::root_solver())
         << "    [Backward] "
         << "Layer " << layer_names_[layer_id]
         << ", param blob " << param_id
         << " diff: " << diff_abs_val_mean;
+#endif
   }
 }
+#endif
 
 template <typename Dtype>
 void Net<Dtype>::UpdateDebugInfo(const int param_id) {
@@ -646,20 +700,24 @@
   const Dtype diff_abs_val_mean = blob.asum_diff() / blob.count();
   if (param_owner < 0) {
     const Dtype data_abs_val_mean = blob.asum_data() / blob.count();
+#ifdef USE_GLOG
     LOG_IF(INFO, Caffe::root_solver())
         << "    [Update] Layer " << layer_name
         << ", param " << param_display_name
         << " data: " << data_abs_val_mean
         << "; diff: " << diff_abs_val_mean;
+#endif
   } else {
     const string& owner_layer_name =
         layer_names_[param_layer_indices_[param_owner].first];
+#ifdef USE_GLOG
     LOG_IF(INFO, Caffe::root_solver())
         << "    [Update] Layer " << layer_name
         << ", param blob " << param_display_name
         << " (owned by layer " << owner_layer_name << ", " << "param "
         << param_display_names_[param_owners_[param_id]] << ")"
         << " diff: " << diff_abs_val_mean;
+#endif
   }
 }
 
@@ -695,6 +753,7 @@
   }
 }
 
+#ifdef ENABLE_BACKWARD
 template <typename Dtype>
 void Net<Dtype>::BackwardFrom(int start) {
   BackwardFromTo(start, 0);
@@ -730,6 +789,7 @@
     layers_[i]->Reshape(bottom_vecs_[i], top_vecs_[i]);
   }
 }
+#endif
 
 template <typename Dtype>
 void Net<Dtype>::CopyTrainedLayersFrom(const NetParameter& param) {
@@ -771,10 +831,14 @@
 
 template <typename Dtype>
 void Net<Dtype>::CopyTrainedLayersFrom(const string trained_filename) {
+#ifdef USE_HDF5
   if (trained_filename.size() >= 3 &&
       trained_filename.compare(trained_filename.size() - 3, 3, ".h5") == 0) {
     CopyTrainedLayersFromHDF5(trained_filename);
   } else {
+#else
+  {
+#endif
     CopyTrainedLayersFromBinaryProto(trained_filename);
   }
 }
@@ -787,8 +851,10 @@
   CopyTrainedLayersFrom(param);
 }
 
+
 template <typename Dtype>
 void Net<Dtype>::CopyTrainedLayersFromHDF5(const string trained_filename) {
+#ifdef USE_HDF5
   hid_t file_hid = H5Fopen(trained_filename.c_str(), H5F_ACC_RDONLY,
                            H5P_DEFAULT);
   CHECK_GE(file_hid, 0) << "Couldn't open " << trained_filename;
@@ -835,6 +901,7 @@
   }
   H5Gclose(data_hid);
   H5Fclose(file_hid);
+#endif
 }
 
 template <typename Dtype>
@@ -851,6 +918,7 @@
 
 template <typename Dtype>
 void Net<Dtype>::ToHDF5(const string& filename, bool write_diff) const {
+#ifdef USE_HDF5
   hid_t file_hid = H5Fcreate(filename.c_str(), H5F_ACC_TRUNC, H5P_DEFAULT,
       H5P_DEFAULT);
   CHECK_GE(file_hid, 0)
@@ -904,14 +972,17 @@
     H5Gclose(diff_hid);
   }
   H5Fclose(file_hid);
+#endif
 }
 
+#ifdef ENABLE_BACKWARD
 template <typename Dtype>
 void Net<Dtype>::Update() {
   for (int i = 0; i < learnable_params_.size(); ++i) {
     learnable_params_[i]->Update();
   }
 }
+#endif
 
 template <typename Dtype>
 void Net<Dtype>::ClearParamDiffs() {
@@ -981,4 +1052,29 @@
 
 INSTANTIATE_CLASS(Net);
 
+#if 0
+// FIXME need?
+/* force register */
+#define FORCE_REG(type) \
+	extern LayerRegistry<float> g_creator_f_##type; \
+	extern LayerRegistry<double> g_creator_d_##type; \
+	LayerRegistry<float> *__g_creator_f_##type = &g_creator_f_##type; \
+	LayerRegistry<double> *__g_creator_d_##type = &g_creator_d_##type
+
+FORCE_REG(TanH);
+FORCE_REG(Pooling);
+FORCE_REG(ReLU);
+FORCE_REG(Sigmoid);
+FORCE_REG(Softmax);
+FORCE_REG(Convolution);
+
+FORCE_REG(Concat);
+FORCE_REG(BNLL);
+FORCE_REG(Flatten);
+FORCE_REG(InnerProduct);
+FORCE_REG(LRN);
+FORCE_REG(MemoryData);
+
+FORCE_REG(Split);
+#endif
 }  // namespace caffe
